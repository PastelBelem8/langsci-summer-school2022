{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day3-Masked-Language-Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 3 - Masked Language Modeling\n",
        "\n",
        "Welcome to day 3! We hope you're ready for some more modeling! \n",
        "\n",
        "We divided the notebook into three main parts, as described below. The first one will be focused on the tokenization and how different models preprocess the text you feed as inputs. In the second part, we will be experimenting with two widely used **masked language model (MLM)** among the NLP community. These models were pretrained on large English corpora using a *fill-in the blank* task. Finally, we will apply the same *fill-in the blank* to a different model and assess its advantages and limitations over the BERT-based models.\n",
        "\n",
        "\n",
        "## Agenda:\n",
        "\n",
        "- Part 1. Tokenization\n",
        "- Part 2. Masked Language Modelling using BERT-based models\n",
        "- Part 3. Masked Language Modelling using T5\n",
        "\n",
        "\n",
        "\n",
        "As usual, the main task in this notebook is to play with the models, find examples that surprise you or are intriguing. If any example challenges your ideas, let us know! ðŸ¤¯\n",
        "\n",
        "Have fun!"
      ],
      "metadata": {
        "id": "zJk58btojT0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Standard installation. We'll use HuggingFace Transformers library. We'll then load some classes that facilitate interactions with the models."
      ],
      "metadata": {
        "id": "TaE_ipEjoAou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjVmCUKzoAHe",
        "outputId": "6f2ab575-0ac9-46cd-da8f-9ad00f114287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 151 ms, sys: 40.6 ms, total: 192 ms\n",
            "Wall time: 16.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertForMaskedLM, pipeline"
      ],
      "metadata": {
        "id": "NMrefR86oZ8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(model, text: str, n=2, wordpieces=None):\n",
        "  \"\"\"Gets predictions for BERT-based models.\"\"\"\n",
        "  orig_text = text\n",
        "  if isinstance(model.model, BertForMaskedLM):\n",
        "    text = text.replace(\"<mask>\", \"[MASK]\")\n",
        "    prefix = \"[bert]\"\n",
        "  else:\n",
        "    prefix = \"[roberta]\"\n",
        "  \n",
        "  results = model(text, targets=wordpieces)\n",
        "\n",
        "  for i, r in enumerate(results):\n",
        "    if i >= n: return\n",
        "\n",
        "    message = f\"{prefix} {orig_text} -----> {r['token_str'].strip()}\"\n",
        "    message += f\"\\t {round(r['score'], 3)}\"\n",
        "    print(message)"
      ],
      "metadata": {
        "id": "3oC_Hyy8vh8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. Tokenization\n",
        "\n",
        "Before we even start using Language Models, we'd like to point out how different models process words. \n",
        "\n",
        "In the next few cells, we will load three different models that you've learned about in class! We will see how they transform texts into different word pieces! "
      ],
      "metadata": {
        "id": "2xTvI8JwF8QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "# Load 3 different tokenization models\n",
        "wordp = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "sentp = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "bytep = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "L4ZTvbeqHCa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9c04a830-a2ce-4fad-e57b-ef7415633acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.04 s, sys: 197 ms, total: 2.24 s\n",
            "Wall time: 9.02 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "We invite you to play around and try out your own (: \n",
        "\n",
        "\n",
        "Which one works best for your use case? \n"
      ],
      "metadata": {
        "id": "QYORt9uMqR2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My name is annoyingly fantastic and egregious.\"\n",
        "print(\"BERT tokenizer:\", wordp.tokenize(text))  # word piece\n",
        "print(\"XLNet tokenizer:\", sentp.tokenize(text)) # sentence piece = word piece + \" \"\n",
        "print(\"GPT2 tokenizer:\", bytep.tokenize(text))  # byte pairs"
      ],
      "metadata": {
        "id": "gObjYP5eGuGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b45e64-05c4-4714-8a2a-e5bb0c8c6f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT tokenizer: ['my', 'name', 'is', 'annoying', '##ly', 'fantastic', 'and', 'e', '##gre', '##gio', '##us', '.']\n",
            "XLNet tokenizer: ['â–My', 'â–name', 'â–is', 'â–annoying', 'ly', 'â–fantastic', 'â–and', 'â–', 'eg', 'reg', 'ious', '.']\n",
            "GPT2 tokenizer: ['My', 'Ä name', 'Ä is', 'Ä annoy', 'ingly', 'Ä fantastic', 'Ä and', 'Ä egregious', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My name is à¤¸à¤®à¥€à¤°\"\n",
        "print(\"BERT tokenizer:\", wordp.tokenize(text))\n",
        "print(\"XLNet tokenizer:\", sentp.tokenize(text))\n",
        "print(\"GPT2 tokenizer:\", bytep.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8-IDIMXGuSu",
        "outputId": "e60f303e-b81f-4fe1-afc0-aee97c9def0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT tokenizer: ['my', 'name', 'is', 'à¤¸', '##à¤®', '##à¥€', '##à¤°']\n",
            "XLNet tokenizer: ['â–My', 'â–name', 'â–is', 'â–', 'à¤¸à¤®à¤°']\n",
            "GPT2 tokenizer: ['My', 'Ä name', 'Ä is', 'Ä Ã Â¤', 'Â¸', 'Ã Â¤', 'Â®', 'Ã Â¥', 'Ä¢', 'Ã Â¤', 'Â°']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Extra] List of BERT Word Pieces\n",
        "\n",
        "If you are curious whether a token is in the BERT vocabulary, you can find the complete list here: https://huggingface.co/bert-base-cased/raw/main/vocab.txt\n",
        "\n",
        "**Warning**: The file is pretty big! ðŸ˜…"
      ],
      "metadata": {
        "id": "OGWKEs4nmcSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. Masked Language Model using BERT-based models\n",
        "\n",
        "In this section, we introduce two different language models - BERT and RoBERTa. Although they have both been trained using a masked language modeling objective, they have some differences in the way they were trained, including the datasets and different hyperparameters.\n"
      ],
      "metadata": {
        "id": "WtIFvekgkPeQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "YBW6KDo421Mg",
        "outputId": "b4d4ed93-a5f5-48a9-a4d7-fcd1ca9eb86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26.4 s, sys: 5.54 s, total: 31.9 s\n",
            "Wall time: 36.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "# Loads the BERT language model\n",
        "bert = pipeline('fill-mask', model='bert-base-uncased')\n",
        "# Loads the RoBERTa language model\n",
        "roberta = pipeline('fill-mask', model='roberta-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand what masked language modeling is, let us walk you through some examples. When training, these models were given *fill-in-the-blank* sentences with randomly masked out words. \n",
        "One such example could be predicting the blank in the following sentence: \n",
        "\n",
        "> my _____ is cute.\n",
        "\n",
        "We will check the guesses of BERT and RoBERTA but before that, what word do you think it should be? ðŸ˜€\n"
      ],
      "metadata": {
        "id": "G36uKpnI5Mq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"2 + 2 = <mask>.\"\n",
        "get_prediction(bert, text, n=5)\n",
        "print()\n",
        "get_prediction(roberta, text, n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqMevRcrx7ze",
        "outputId": "61f7d776-80d0-49d3-9efc-0fd68e66ec9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bert] 2 + 2 = <mask>. -----> 0\t 0.11\n",
            "[bert] 2 + 2 = <mask>. -----> 2\t 0.104\n",
            "[bert] 2 + 2 = <mask>. -----> 3\t 0.09\n",
            "[bert] 2 + 2 = <mask>. -----> 1\t 0.088\n",
            "[bert] 2 + 2 = <mask>. -----> 4\t 0.052\n",
            "\n",
            "[roberta] 2 + 2 = <mask>. -----> 2\t 0.212\n",
            "[roberta] 2 + 2 = <mask>. -----> 1\t 0.138\n",
            "[roberta] 2 + 2 = <mask>. -----> 3\t 0.13\n",
            "[roberta] 2 + 2 = <mask>. -----> 4\t 0.126\n",
            "[roberta] 2 + 2 = <mask>. -----> 0\t 0.086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is weird that BERT has assigned **mom** and **dad** the highest. Let us see what would be the value for other tokens like **cat** and **dog**."
      ],
      "metadata": {
        "id": "UFo0uhiZzUOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(bert, text, n=2, wordpieces=[\"dog\", \"cat\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyVtuOs8zTjv",
        "outputId": "33a67327-3216-4440-b494-c32761992780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bert] My <mask> is cute. -----> dog\t 0.003\n",
            "[bert] My <mask> is cute. -----> cat\t 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uff! That's low!"
      ],
      "metadata": {
        "id": "p2aNEXy1zqCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A Sentiment Classification example\n",
        "\n",
        "Let us now try something more challenging. Sentiment classification is the task where we provide a review (of movie, book, food, hotel, etc) and ask the model to predict whether it is a positive or a bad review."
      ],
      "metadata": {
        "id": "VQzkJRvBx77T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The movie was a lot of fun. I this this movie was <mask>.\"\n",
        "\n",
        "get_prediction(bert, text, n=5)\n",
        "print()\n",
        "get_prediction(roberta, text, n=5)\n",
        "\n",
        "print(\"\"\"\\n\n",
        "# -------------------------------------------------------------\n",
        "# Check scores of some specific words by setting wordpieces\n",
        "# -------------------------------------------------------------\\n\n",
        "\"\"\"\n",
        ")\n",
        "word_subset = [\"good\", \"love\", \"hate\", \"bad\"]\n",
        "\n",
        "get_prediction(bert, text, n=5, wordpieces=word_subset)\n",
        "print()\n",
        "# get_prediction(roberta, text, n=5, wordpieces=word_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnskA6p_0RI9",
        "outputId": "83e402da-404a-4a77-d26b-db9f936e1953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> fun\t 0.248\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> good\t 0.105\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> great\t 0.096\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> awesome\t 0.076\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> amazing\t 0.048\n",
            "\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> good\t 0.296\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> great\t 0.154\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> fun\t 0.114\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> awesome\t 0.109\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> funny\t 0.029\n",
            "\n",
            "\n",
            "# -------------------------------------------------------------\n",
            "# Check scores of some specific words by setting wordpieces\n",
            "# -------------------------------------------------------------\n",
            "\n",
            "\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> good\t 0.105\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> bad\t 0.007\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> love\t 0.001\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> hate\t 0.0\n",
            "\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> good\t 0.0\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> bad\t 0.0\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> love\t 0.0\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> hate\t 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The movie was a lot of fun. I think this movie was <mask>.\"\n",
        "word_subset = [\"good\", \"love\", \"hate\", \"bad\"]\n",
        "\n",
        "get_prediction(bert, text, n=5, wordpieces=word_subset)\n",
        "print()\n",
        "get_prediction(roberta, text, n=5, wordpieces=word_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpZnGDbj7iKV",
        "outputId": "98af143e-b21e-469e-8557-203691e46248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> good\t 0.105\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> bad\t 0.007\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> love\t 0.001\n",
            "[bert] The movie was a lot of fun. I this this movie was <mask>. -----> hate\t 0.0\n",
            "\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> good\t 0.0\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> bad\t 0.0\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> love\t 0.0\n",
            "[roberta] The movie was a lot of fun. I this this movie was <mask>. -----> hate\t 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The acting and directing was terrible. I <mask> this movie.\"\n",
        "get_prediction(bert, text, n=5)\n",
        "print()\n",
        "get_prediction(roberta, text, n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQGS-Vwd7iM0",
        "outputId": "71ff28f7-3474-4f19-bbe6-025999b7ffee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bert] The acting and directing was terrible. I <mask> this movie. -----> loved\t 0.513\n",
            "[bert] The acting and directing was terrible. I <mask> this movie. -----> love\t 0.14\n",
            "[bert] The acting and directing was terrible. I <mask> this movie. -----> hated\t 0.11\n",
            "[bert] The acting and directing was terrible. I <mask> this movie. -----> liked\t 0.054\n",
            "[bert] The acting and directing was terrible. I <mask> this movie. -----> enjoyed\t 0.045\n",
            "\n",
            "[roberta] The acting and directing was terrible. I <mask> this movie. -----> hated\t 0.738\n",
            "[roberta] The acting and directing was terrible. I <mask> this movie. -----> hate\t 0.092\n",
            "[roberta] The acting and directing was terrible. I <mask> this movie. -----> loved\t 0.066\n",
            "[roberta] The acting and directing was terrible. I <mask> this movie. -----> disliked\t 0.029\n",
            "[roberta] The acting and directing was terrible. I <mask> this movie. -----> liked\t 0.017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: \n",
        "Each blank is replaced with the token `<mask>` as shown above!\n",
        "\n",
        "### Exercise\n",
        "\n",
        "Play around with the models and try to find cases where they fail but humans wouldn't. For example, would you expect the models to return the same output for the following prompts? Why? Why not?\n",
        "\n",
        "- `\"my <mask> is cute.\"`\n",
        "- `\"my <mask> is cute\"`\n",
        "\n"
      ],
      "metadata": {
        "id": "jRGdw10O8D9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breaking Masked Language Models\n",
        "\n",
        "We have seen in class that language modeling pretraining constitutes a laborous process that goes through large amount of textual data and tries to predict the right word for randomly *fill-in-the-blank* sentences. As a consequence, the model ends up learning word co-occurrences in English. \n",
        "As such, we might observe big changes in the model's outputs when small changes to the input. "
      ],
      "metadata": {
        "id": "Dorxhgjv7hul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Warmup \n",
        "\n",
        "In this part, of the notebook we ask you to find some prompts where the model's outputs differ significantly. You can choose a single model or compare the outputs of the different models. Are both models equally brittle to these variations or is there one that is more robust?"
      ],
      "metadata": {
        "id": "-hXhxPKK-1pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ... # TODO: decide whether you want to use bert or roberta\n",
        "text = ... # TODO: write the text\n",
        "\n",
        "get_prediction(model, text, n=5)"
      ],
      "metadata": {
        "id": "4pOw7VcYxFvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1. Finding evidence of Bias and Discrimination\n",
        "\n",
        "A common concern nowadays is whether models are perpetuating biases and unwittingly causing harm to certain population groups. In this exercise, we ask you to come up with different *fill-in-the-blank* sentences that you would expect the model to be (or not to be) biased.\n",
        "\n",
        "\n",
        "**Hint**: Few such examples could be `\"the doctor said <mask> was busy tonight.\"` or `\"<mask> is a mechanic.\"`. What do you expect it to be? Does the model meet your expectations? Why (why not)? "
      ],
      "metadata": {
        "id": "07Z6i0s4yT0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ... # TODO: decide whether you want to use bert or roberta\n",
        "text = ... # TODO: write the text\n",
        "\n",
        "get_prediction(model, text, n=5)"
      ],
      "metadata": {
        "id": "jIdCO-TFvu2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Playing around with job occupations might be a fun starting point but try to come up with other examples. Can you find examples of discrimination of other socio-demographic groups?"
      ],
      "metadata": {
        "id": "NCWohapGB2dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Try writing down other fill in the blank sentences\n",
        "# Examples: \"<mask> is a mechanic.\""
      ],
      "metadata": {
        "id": "tF4LwMGdBi1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2. Know your facts\n",
        "\n",
        "Despite the biases, these models are quite powerful! In fact, they can serve as knowledge bases. For instance, if you wish to know where Barack Obama was born, we'll have a RoBERTA or a BERT model correctly providing you that information.\n",
        "\n"
      ],
      "metadata": {
        "id": "HAvt8L7MyWbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction(roberta, \"Obama was born in <mask>.\", n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myQZlxTFyWh-",
        "outputId": "14526411-a893-4af0-d75f-2a87790c43ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[roberta] Obama was born in <mask>. -----> Hawaii\t 0.135\n",
            "[roberta] Obama was born in <mask>. -----> Chicago\t 0.127\n",
            "[roberta] Obama was born in <mask>. -----> Kenya\t 0.099\n",
            "[roberta] Obama was born in <mask>. -----> 1963\t 0.035\n",
            "[roberta] Obama was born in <mask>. -----> 1961\t 0.033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your job is to **find impressive examples** or some examples where the models fail. Do both models fail or just one? \n"
      ],
      "metadata": {
        "id": "KNI9pmg1ERjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ... # TODO: decide whether you want to use bert or roberta\n",
        "text = ... # TODO: write the text\n",
        "\n",
        "get_prediction(model, text) # TODO: "
      ],
      "metadata": {
        "id": "iMUh1SHIEgkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 3. The sky is the \\<mask>\n",
        "\n",
        "In this section, we invite you to try different things, from common sense to temporal or numerical reasoning, we want to know what you find about the models.\n",
        "\n",
        "\n",
        "As a few examples to get you running, consider templates like: \n",
        "\n",
        "- `two plus two is <mask>`\n",
        "- `Elephants are much <mask> than mice.`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ck_WlCazEui2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ... # TODO: decide whether you want to use bert or roberta\n",
        "text = ... # TODO: write the text\n",
        "\n",
        "get_prediction(model, text) # TODO: "
      ],
      "metadata": {
        "id": "84DYeySwEuou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3. What if we had more flexibility in Masked Language Modeling? \n",
        "\n",
        "In this section, we will interact with a model that although it was not trained using a masked language modeling objective, it can also interact via *fill-in-the-blank* prompts.  \n",
        "\n",
        "This model is known as T5 and is trained using a causal language modeling. Therefore, it does not take benefit from the future tokens."
      ],
      "metadata": {
        "id": "tEphiqaqWpJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and define helper methods"
      ],
      "metadata": {
        "id": "HWvQ32mxa16z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEHWI09_ctJ6",
        "outputId": "cf6b8b99-79f5-44ef-ff9f-c25a7f6af131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 92.5 ms, sys: 111 ms, total: 204 ms\n",
            "Wall time: 8.28 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "MASK_PATTERN = r\"<extra_id_[0-9]{1,2}>\"\n",
        "\n",
        "def get_masks(text: str) -> dict:\n",
        "  \"\"\"Given a text return the sentinel masks ``<extra_id_[0-9]{1,2}>``.\"\"\"\n",
        "  mask_values = re.split(MASK_PATTERN, text)\n",
        "  mask_values = [m for m in mask_values if m]\n",
        "  results = {}\n",
        "  for i, val in enumerate(mask_values):\n",
        "    results[f\"<extra_id_{i}>\"] = val\n",
        "  \n",
        "  return results  \n",
        "\n",
        "\n",
        "def replace_masks(text_orig: str, masked_outputs: list):\n",
        "  \"\"\"Replace the masks in the original text with the masked outputs.\"\"\"\n",
        "  final_texts = []\n",
        "\n",
        "  for out in masked_outputs:\n",
        "    text = text_orig\n",
        "    # Get the masks in the model output \n",
        "    # (essentially scan for <extra_id_N> tags)\n",
        "    masks = get_masks(out)\n",
        "\n",
        "    # Replace each mask in the original text\n",
        "    for mask_name, mask_value in masks.items():\n",
        "      # Some tokens have some whitespace surrounding it\n",
        "      mask_value = mask_value.strip()\n",
        "      text = text.replace(mask_name, mask_value)\n",
        "\n",
        "    final_texts.append(text)\n",
        "  return final_texts\n",
        "\n",
        "def to_t5_mask_format(text: str) -> str:\n",
        "  fragments = text.split(\"<mask>\")\n",
        "  t5_masked = []\n",
        "\n",
        "  for i, frag in enumerate(fragments):\n",
        "    t5_masked.append(frag)\n",
        "\n",
        "    if i < len(fragments)-1:\n",
        "      t5_masked.append(f\"<extra_id_{i}>\")\n",
        "\n",
        "  text = \" \".join(t5_masked)\n",
        "  return text\n",
        "\n",
        "def generate_input(text, model, tokenizer, n=5, max_length=5, num_beams=200, debug=False):\n",
        "  # Get T5 mask format using sentinels <extra_id_n>\n",
        "  masked_text = to_t5_mask_format(text)\n",
        "  if debug:\n",
        "    print(masked_text)\n",
        "\n",
        "  # Encode the masked text\n",
        "  encoded = tokenizer.encode_plus(masked_text, add_special_tokens=True, return_tensors='pt')\n",
        "\n",
        "  # Generating `n` sequences with maximum length set to `max_length`\n",
        "  outputs = model.generate(input_ids=encoded['input_ids'],\n",
        "                           num_beams=num_beams,\n",
        "                           num_return_sequences=n,\n",
        "                           max_length=max_length)\n",
        "  \n",
        "  # Note: skip_special_tokens=False, so that we preserve the sentinels\n",
        "  # and can do the proper parsing of the string.\n",
        "  outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "  # Remove the pad token from the output (if present)\n",
        "  outputs = [out.replace(\"<pad> \", \"\") for out in outputs]\n",
        "\n",
        "  outputs = replace_masks(masked_text, outputs)\n",
        "  for i, out in enumerate(outputs):\n",
        "    print(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "huFKcf4UbYn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text with T5 \n",
        "\n",
        "\n",
        "In this section, we'll try to evoke some strings from T5, an autoregressive model. While it does not benefit from the same *bidirectional* view as the other **masked language models**, it can still output some texts when provided a fill-in the blank kind of prompt. \n",
        "\n",
        "Let's try some, shall we? (: \n",
        "\n",
        "\n",
        "First, let us download a pre-trained T5 model! We're starting small to make sure everything is up and running appropriately. Feel free to use the base version instead by specifyin `t5-base`. \n",
        "\n",
        "**Note that this will make your generations more interesting but also much slower!**"
      ],
      "metadata": {
        "id": "PtIq9Nt9cNAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "model_name = 't5-base' # \"t5-small\", \"t5-base\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ay-MH_hLNCXY",
        "outputId": "649c0293-5039-49a6-bd47-b24a728dfcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 24.2 s, sys: 4.53 s, total: 28.8 s\n",
            "Wall time: 37.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just provide the fill-in the blank prompts, we'd like! One key aspect, is that T5 model is no longer limited to a single word piece but instead can generate multiple word pieces for the same mask! This is due to its auto-regressive nature!"
      ],
      "metadata": {
        "id": "wRQjeYFlgfR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I <mask> NLP!\"\n",
        "generate_input(text, model=t5_model, tokenizer=t5_tokenizer)"
      ],
      "metadata": {
        "id": "X5DERzh3U1wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a46bc3c-cdd6-4e5f-d587-dda440bc222d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I  love  NLP!\n",
            "I  LOVE  NLP!\n",
            "I  love  NLP!\n",
            "I  NEED  NLP!\n",
            "I  LOVE  NLP!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise \n",
        "\n",
        "Like before, just play around with some patterns and try to see what you can get. Can you break the model? :)"
      ],
      "metadata": {
        "id": "yBrM_LLmmwKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can frame it more similarly to other NLP tasks, like Sentiment Classification. If you're a foodie, you can give it a try ðŸ˜œ Give us your wildest review and let us see what T5 has to say about that!"
      ],
      "metadata": {
        "id": "cNpSP9qanslo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The movie was a lot of fun. I <mask> this movie.\"\n",
        "generate_input(text, model=t5_model, tokenizer=t5_tokenizer, max_length=10)\n",
        "\n",
        "print()\n",
        "\n",
        "text = \"The acting and directing was terrible. I <mask> this movie.\"\n",
        "generate_input(text, model=t5_model, tokenizer=t5_tokenizer, max_length=10)"
      ],
      "metadata": {
        "id": "1RMLXLiDWpU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ebefbd-a7cf-49f4-9e0d-5ec19d79a77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie was a lot of fun. I  really enjoyed  this movie.\n",
            "The movie was a lot of fun. I  loved  this movie.\n",
            "The movie was a lot of fun. I  highly recommend  this movie.\n",
            "The movie was a lot of fun. I  enjoyed  this movie.\n",
            "The movie was a lot of fun. I  really enjoyed  this movie.\n",
            "\n",
            "The acting and directing was terrible. I  didn't like  this movie.\n",
            "The acting and directing was terrible. I  didn't like  this movie.\n",
            "The acting and directing was terrible. I  did not like  this movie.\n",
            "The acting and directing was terrible. I  didn't like  this movie.\n",
            "The acting and directing was terrible. I  didn't like  this movie.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Teaser for tomorrow's class\n",
        "\n",
        "If you'd like to explore a little bit a different class of Language Models, head over to [HuggingFace t5-small playground](https://huggingface.co/t5-small)."
      ],
      "metadata": {
        "id": "Hx8bcqyhKkfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources\n",
        "\n",
        "- [HuggingFace Documentation on T5: Training section](https://huggingface.co/docs/transformers/model_doc/t5#training)\n",
        "- [HuggingFace Github Issue](https://github.com/huggingface/transformers/issues/3985)"
      ],
      "metadata": {
        "id": "FJFJnR8jL7ro"
      }
    }
  ]
}